<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Research Interview - ICIScopilot</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%);
      min-height: 100vh;
      color: #e2e8f0;
    }

    .container {
      max-width: 800px;
      margin: 0 auto;
      padding: 2rem;
    }

    .header {
      text-align: center;
      margin-bottom: 2rem;
    }

    .header h1 {
      font-size: 2rem;
      color: #10b981;
      margin-bottom: 0.5rem;
    }

    .header p {
      color: #94a3b8;
      font-size: 0.9rem;
    }

    .card {
      background: #1e293b;
      border-radius: 1rem;
      padding: 2rem;
      margin-bottom: 1.5rem;
      border: 1px solid #334155;
    }

    .form-group {
      margin-bottom: 1.5rem;
    }

    .form-group label {
      display: block;
      margin-bottom: 0.5rem;
      font-weight: 500;
      color: #cbd5e1;
    }

    .form-group input,
    .form-group textarea {
      width: 100%;
      padding: 0.75rem 1rem;
      border-radius: 0.5rem;
      border: 1px solid #475569;
      background: #0f172a;
      color: #e2e8f0;
      font-size: 1rem;
    }

    .form-group input:focus,
    .form-group textarea:focus {
      outline: none;
      border-color: #10b981;
    }

    .form-group textarea {
      min-height: 100px;
      resize: vertical;
    }

    .btn {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      gap: 0.5rem;
      padding: 0.75rem 1.5rem;
      border-radius: 0.5rem;
      font-size: 1rem;
      font-weight: 500;
      cursor: pointer;
      border: none;
      transition: all 0.2s;
    }

    .btn-primary {
      background: #10b981;
      color: white;
    }

    .btn-primary:hover {
      background: #059669;
    }

    .btn-primary:disabled {
      background: #475569;
      cursor: not-allowed;
    }

    .btn-danger {
      background: #ef4444;
      color: white;
    }

    .btn-danger:hover {
      background: #dc2626;
    }

    .btn-secondary {
      background: #475569;
      color: white;
    }

    .btn-secondary:hover {
      background: #64748b;
    }

    .interview-status {
      text-align: center;
      padding: 2rem;
    }

    .status-indicator {
      width: 120px;
      height: 120px;
      border-radius: 50%;
      margin: 0 auto 1.5rem;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 3rem;
    }

    .status-indicator.listening {
      background: rgba(239, 68, 68, 0.2);
      border: 3px solid #ef4444;
      animation: pulse 1.5s infinite;
    }

    .status-indicator.speaking {
      background: rgba(16, 185, 129, 0.2);
      border: 3px solid #10b981;
      animation: pulse 1s infinite;
    }

    .status-indicator.idle {
      background: rgba(148, 163, 184, 0.2);
      border: 3px solid #94a3b8;
    }

    @keyframes pulse {
      0%, 100% { transform: scale(1); opacity: 1; }
      50% { transform: scale(1.05); opacity: 0.8; }
    }

    .status-text {
      font-size: 1.25rem;
      margin-bottom: 0.5rem;
    }

    .status-subtext {
      color: #94a3b8;
      font-size: 0.9rem;
    }

    .transcript-box {
      background: #0f172a;
      border-radius: 0.5rem;
      padding: 1rem;
      max-height: 300px;
      overflow-y: auto;
      font-family: monospace;
      font-size: 0.85rem;
      line-height: 1.6;
      white-space: pre-wrap;
      border: 1px solid #334155;
    }

    .transcript-box .ai {
      color: #10b981;
    }

    .transcript-box .user {
      color: #60a5fa;
    }

    .controls {
      display: flex;
      gap: 1rem;
      justify-content: center;
      margin-top: 1.5rem;
    }

    .timer {
      font-size: 2rem;
      font-weight: bold;
      color: #10b981;
      text-align: center;
      margin-bottom: 1rem;
    }

    .hidden {
      display: none !important;
    }

    .error-box {
      background: rgba(239, 68, 68, 0.1);
      border: 1px solid #ef4444;
      border-radius: 0.5rem;
      padding: 1rem;
      color: #fca5a5;
      margin-bottom: 1rem;
    }

    .info-box {
      background: rgba(59, 130, 246, 0.1);
      border: 1px solid #3b82f6;
      border-radius: 0.5rem;
      padding: 1rem;
      color: #93c5fd;
      margin-bottom: 1rem;
      font-size: 0.9rem;
    }

    .complete-box {
      text-align: center;
      padding: 2rem;
    }

    .complete-box h2 {
      color: #10b981;
      margin-bottom: 1rem;
    }
  </style>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>ICIScopilot Research Interview</h1>
      <p>Voice-based interview for automated science discovery</p>
    </div>

    <!-- Welcome Screen -->
    <div id="welcome-screen" class="card">
      <h2 style="margin-bottom: 1rem;">Welcome to Your Research Interview</h2>
      <p style="color: #94a3b8; margin-bottom: 1.5rem;">
        This is a structured 9-minute voice interview to understand your research needs.
        Please provide your details below, then click "Start Interview" to begin the voice conversation.
      </p>

      <div class="info-box">
        <strong>How it works:</strong> After you start, an AI interviewer will ask you questions about your research.
        Speak naturally - the AI uses voice activity detection to know when you're done speaking.
        At the end, a transcript will be automatically downloaded.
      </div>

      <div class="form-group">
        <label for="email">Email Address *</label>
        <input type="email" id="email" placeholder="your.email@university.edu" required>
      </div>

      <div class="form-group">
        <label for="references">Key References (1-3 papers that inspire your research)</label>
        <textarea id="references" placeholder="e.g., Smith et al. 2023 - 'AI in Healthcare'&#10;Johnson 2022 - 'Machine Learning Methods'"></textarea>
      </div>

      <div id="error-message" class="error-box hidden"></div>

      <button id="start-btn" class="btn btn-primary" onclick="startInterview()">
        Start Voice Interview
      </button>
    </div>

    <!-- Interview Screen -->
    <div id="interview-screen" class="card hidden">
      <div class="timer" id="timer">09:00</div>

      <div class="interview-status">
        <div class="status-indicator idle" id="status-indicator">
          <span id="status-icon">üéôÔ∏è</span>
        </div>
        <div class="status-text" id="status-text">Connecting...</div>
        <div class="status-subtext" id="status-subtext">Please wait while we set up the interview</div>
      </div>

      <h3 style="margin-bottom: 0.5rem;">Live Transcript</h3>
      <div class="transcript-box" id="transcript"></div>

      <div class="controls">
        <button class="btn btn-danger" onclick="endInterview()">
          End Interview
        </button>
      </div>
    </div>

    <!-- Complete Screen -->
    <div id="complete-screen" class="card hidden">
      <div class="complete-box">
        <h2>Interview Complete!</h2>
        <p style="color: #94a3b8; margin-bottom: 1.5rem;">
          Thank you for completing the research interview. Your transcript has been downloaded automatically.
        </p>

        <h3 style="margin-bottom: 0.5rem;">Final Transcript</h3>
        <div class="transcript-box" id="final-transcript" style="max-height: 400px;"></div>

        <div class="controls" style="margin-top: 1.5rem;">
          <button class="btn btn-secondary" onclick="downloadTranscript()">
            Download Transcript Again
          </button>
          <button class="btn btn-primary" onclick="window.close()">
            Close Window
          </button>
        </div>
      </div>
    </div>
  </div>

  <script>
    // Interview state
    let ws = null;
    let audioContext = null;
    let mediaStream = null;
    let audioWorklet = null;
    let isRecording = false;
    let transcript = '';
    let userEmail = '';
    let userReferences = '';
    let interviewStartTime = null;
    let timerInterval = null;
    let currentSpeaker = null;

    // Interview prompt (embedded)
    const INTERVIEW_PROMPT = `You are conducting a structured research interview for automated scientific discovery. Your goal is to understand the participant's research needs, data, and goals.

INTERVIEW STRUCTURE (Target: 9 minutes total):

1. INTRODUCTION (30 sec)
- Greet warmly and introduce yourself as a research assistant
- Explain the interview will help understand their research needs
- Ask them to speak naturally

2. RESEARCH BACKGROUND * (1 min)
- What is your field of research?
- What specific problem are you trying to solve?

3. DATA DESCRIPTION * (1.5 min)
- What data do you have or plan to collect?
- What format is your data in (CSV, surveys, experiments)?
- How many observations/participants do you have?

4. RESEARCH QUESTIONS * (1.5 min)
- What is your main research question or hypothesis?
- Are there secondary questions you want to explore?

5. METHODOLOGY PREFERENCES (1 min)
- Do you have specific statistical methods in mind?
- Any constraints on the analysis approach?

6. EXPECTED OUTCOMES * (1 min)
- What would a successful result look like?
- How will you use the findings?

7. TIMELINE AND CONSTRAINTS (30 sec)
- Any deadlines or time constraints?
- Resource limitations?

8. RELATED WORK (30 sec)
- Are there similar studies you're building on?
- Key papers that inspire this work?

9. ADDITIONAL CONTEXT (30 sec)
- Anything else important for understanding your research?

10. WRAP-UP (30 sec)
- Summarize key points
- Thank them for their time

GUIDELINES:
- Questions marked with * are mandatory - must be covered
- Use Voice Activity Detection - wait for natural pauses before responding
- Be conversational and supportive, not interrogative
- Ask follow-up questions when answers are unclear
- Keep track of time and gently move forward if running long
- At the end, provide a brief summary of what you learned

IMPORTANT: This is a voice conversation. Speak naturally and conversationally. Do not use markdown, bullet points, or formatting - just natural speech.`;

    // Get API base URL
    function getApiUrl() {
      // If on GitHub Pages, use Vercel API
      if (window.location.hostname.includes('github.io')) {
        return 'https://icis-deploy-12-10-2025.vercel.app';
      }
      // Otherwise use same origin (for Vercel deployment)
      return window.location.origin;
    }

    // Show error message
    function showError(message) {
      const errorBox = document.getElementById('error-message');
      errorBox.textContent = message;
      errorBox.classList.remove('hidden');
    }

    // Hide error message
    function hideError() {
      document.getElementById('error-message').classList.add('hidden');
    }

    // Update status display
    function updateStatus(status, text, subtext) {
      const indicator = document.getElementById('status-indicator');
      const statusText = document.getElementById('status-text');
      const statusSubtext = document.getElementById('status-subtext');
      const icon = document.getElementById('status-icon');

      indicator.className = 'status-indicator ' + status;
      statusText.textContent = text;
      statusSubtext.textContent = subtext;

      if (status === 'listening') {
        icon.textContent = 'üéôÔ∏è';
      } else if (status === 'speaking') {
        icon.textContent = 'üîä';
      } else {
        icon.textContent = '‚è≥';
      }
    }

    // Update timer display
    function updateTimer() {
      if (!interviewStartTime) return;

      const elapsed = Math.floor((Date.now() - interviewStartTime) / 1000);
      const remaining = Math.max(0, 9 * 60 - elapsed);
      const minutes = Math.floor(remaining / 60);
      const seconds = remaining % 60;

      document.getElementById('timer').textContent =
        `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;

      // Auto-end after 10 minutes
      if (elapsed >= 10 * 60) {
        endInterview();
      }
    }

    // Add to transcript
    function addToTranscript(speaker, text) {
      const transcriptBox = document.getElementById('transcript');
      const timestamp = new Date().toLocaleTimeString();

      if (speaker === 'AI') {
        transcript += `\n[${timestamp}] AI: ${text}`;
        transcriptBox.innerHTML += `<div class="ai">[${timestamp}] AI: ${text}</div>`;
      } else {
        transcript += `\n[${timestamp}] You: ${text}`;
        transcriptBox.innerHTML += `<div class="user">[${timestamp}] You: ${text}</div>`;
      }

      transcriptBox.scrollTop = transcriptBox.scrollHeight;
    }

    // Download transcript as text file
    function downloadTranscript() {
      const header = `Research Interview Transcript
Date: ${new Date().toLocaleString()}
Email: ${userEmail}
References: ${userReferences || 'None provided'}
${'='.repeat(50)}
`;

      const content = header + transcript;
      const blob = new Blob([content], { type: 'text/plain' });
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.href = url;
      a.download = `research_interview_${new Date().toISOString().split('T')[0]}.txt`;
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
      URL.revokeObjectURL(url);
    }

    // Start the interview
    async function startInterview() {
      hideError();

      userEmail = document.getElementById('email').value.trim();
      userReferences = document.getElementById('references').value.trim();

      if (!userEmail) {
        showError('Please enter your email address.');
        return;
      }

      if (!userEmail.includes('@')) {
        showError('Please enter a valid email address.');
        return;
      }

      document.getElementById('start-btn').disabled = true;
      document.getElementById('start-btn').textContent = 'Connecting...';

      try {
        // Get API key from server
        const tokenResponse = await fetch(`${getApiUrl()}/api/get-interview-token`);
        const tokenData = await tokenResponse.json();

        if (!tokenData.success) {
          throw new Error(tokenData.error || 'Failed to get API token');
        }

        // Request microphone access
        mediaStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true
          }
        });

        // Initialize audio context
        audioContext = new AudioContext({ sampleRate: 24000 });

        // Show interview screen
        document.getElementById('welcome-screen').classList.add('hidden');
        document.getElementById('interview-screen').classList.remove('hidden');

        // Connect to Gemini Live API
        await connectToGemini(tokenData.apiKey);

      } catch (error) {
        console.error('Failed to start interview:', error);
        showError(`Failed to start: ${error.message}`);
        document.getElementById('start-btn').disabled = false;
        document.getElementById('start-btn').textContent = 'Start Voice Interview';
      }
    }

    // Connect to Gemini Live API via WebSocket
    async function connectToGemini(apiKey) {
      updateStatus('idle', 'Connecting...', 'Establishing connection to AI');

      const model = 'gemini-2.0-flash-exp';
      const wsUrl = `wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent?key=${apiKey}`;

      ws = new WebSocket(wsUrl);

      ws.onopen = () => {
        console.log('WebSocket connected');

        // Send setup message
        const setupMessage = {
          setup: {
            model: `models/${model}`,
            generationConfig: {
              responseModalities: ['AUDIO'],
              speechConfig: {
                voiceConfig: {
                  prebuiltVoiceConfig: {
                    voiceName: 'Aoede'
                  }
                }
              }
            },
            systemInstruction: {
              parts: [{ text: INTERVIEW_PROMPT }]
            }
          }
        };

        ws.send(JSON.stringify(setupMessage));
      };

      ws.onmessage = async (event) => {
        try {
          let data;
          if (event.data instanceof Blob) {
            const text = await event.data.text();
            data = JSON.parse(text);
          } else {
            data = JSON.parse(event.data);
          }

          await handleGeminiMessage(data);
        } catch (error) {
          console.error('Error processing message:', error);
        }
      };

      ws.onerror = (error) => {
        console.error('WebSocket error:', error);
        updateStatus('idle', 'Connection Error', 'Please try again');
      };

      ws.onclose = () => {
        console.log('WebSocket closed');
        if (isRecording) {
          stopRecording();
        }
      };
    }

    // Handle messages from Gemini
    async function handleGeminiMessage(data) {
      // Setup complete
      if (data.setupComplete) {
        console.log('Setup complete, starting interview');
        interviewStartTime = Date.now();
        timerInterval = setInterval(updateTimer, 1000);

        // Start recording
        startRecording();

        // Send initial message to start the interview
        const contextMessage = userReferences
          ? `The participant's email is ${userEmail}. They mentioned these references as inspiration: ${userReferences}. Please begin the interview.`
          : `The participant's email is ${userEmail}. Please begin the interview.`;

        sendTextMessage(contextMessage);
        return;
      }

      // Handle server content (audio response)
      if (data.serverContent) {
        const parts = data.serverContent.modelTurn?.parts || [];

        for (const part of parts) {
          // Handle audio
          if (part.inlineData?.mimeType?.startsWith('audio/')) {
            updateStatus('speaking', 'AI Speaking', 'Listening to response...');
            await playAudio(part.inlineData.data);
          }

          // Handle text (for transcript)
          if (part.text) {
            addToTranscript('AI', part.text);
          }
        }

        // Check if turn is complete
        if (data.serverContent.turnComplete) {
          updateStatus('listening', 'Your Turn', 'Speak when ready...');
        }
      }

      // Handle tool calls (if any)
      if (data.toolCall) {
        console.log('Tool call received:', data.toolCall);
      }
    }

    // Send text message to Gemini
    function sendTextMessage(text) {
      if (!ws || ws.readyState !== WebSocket.OPEN) return;

      const message = {
        clientContent: {
          turns: [{
            role: 'user',
            parts: [{ text }]
          }],
          turnComplete: true
        }
      };

      ws.send(JSON.stringify(message));
    }

    // Track if AI is currently speaking (to prevent echo)
    let isAISpeaking = false;

    // Start recording audio
    async function startRecording() {
      if (!mediaStream) return;

      // Create a separate audio context for recording at 16kHz
      const recordingContext = new AudioContext({ sampleRate: 16000 });

      isRecording = true;
      updateStatus('listening', 'Listening', 'Speak when ready...');

      const source = recordingContext.createMediaStreamSource(mediaStream);

      // Create a script processor for audio capture
      const processor = recordingContext.createScriptProcessor(4096, 1, 1);

      processor.onaudioprocess = (e) => {
        // Don't send audio while AI is speaking (prevents echo)
        if (!isRecording || !ws || ws.readyState !== WebSocket.OPEN || isAISpeaking) return;

        const inputData = e.inputBuffer.getChannelData(0);

        // Convert to 16-bit PCM (already at 16kHz from the recording context)
        const pcmData = float32ToPcm16(inputData);

        // Convert to base64
        const base64 = arrayBufferToBase64(pcmData.buffer);

        // Send to Gemini
        const audioMessage = {
          realtimeInput: {
            mediaChunks: [{
              mimeType: 'audio/pcm;rate=16000',
              data: base64
            }]
          }
        };

        ws.send(JSON.stringify(audioMessage));
      };

      source.connect(processor);
      // Connect to a dummy destination (required for processing to work, but no audio output)
      processor.connect(recordingContext.destination);

      audioWorklet = { source, processor, context: recordingContext };
    }

    // Stop recording
    function stopRecording() {
      isRecording = false;

      if (audioWorklet) {
        audioWorklet.source.disconnect();
        audioWorklet.processor.disconnect();
        if (audioWorklet.context) {
          audioWorklet.context.close();
        }
        audioWorklet = null;
      }
    }

    // Audio queue for sequential playback
    let audioQueue = [];
    let isPlayingAudio = false;

    // Play audio from base64
    async function playAudio(base64Data) {
      if (!audioContext) return;

      // Add to queue
      audioQueue.push(base64Data);

      // Process queue if not already playing
      if (!isPlayingAudio) {
        processAudioQueue();
      }
    }

    // Process audio queue sequentially
    async function processAudioQueue() {
      if (audioQueue.length === 0) {
        isPlayingAudio = false;
        isAISpeaking = false;
        return;
      }

      isPlayingAudio = true;
      isAISpeaking = true;

      const base64Data = audioQueue.shift();

      try {
        // Decode base64 to ArrayBuffer
        const binaryString = atob(base64Data);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }

        // Convert PCM to AudioBuffer (24kHz, 16-bit, mono)
        const pcmData = new Int16Array(bytes.buffer);
        const floatData = new Float32Array(pcmData.length);

        for (let i = 0; i < pcmData.length; i++) {
          floatData[i] = pcmData[i] / 32768;
        }

        const audioBuffer = audioContext.createBuffer(1, floatData.length, 24000);
        audioBuffer.getChannelData(0).set(floatData);

        // Play the audio
        const source = audioContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(audioContext.destination);

        // When this chunk finishes, process next in queue
        source.onended = () => {
          processAudioQueue();
        };

        source.start();

      } catch (error) {
        console.error('Error playing audio:', error);
        // Continue with next chunk even on error
        processAudioQueue();
      }
    }

    // Utility: Resample audio
    function resample(inputData, inputRate, outputRate) {
      if (inputRate === outputRate) return inputData;

      const ratio = inputRate / outputRate;
      const outputLength = Math.floor(inputData.length / ratio);
      const output = new Float32Array(outputLength);

      for (let i = 0; i < outputLength; i++) {
        const inputIndex = i * ratio;
        const index = Math.floor(inputIndex);
        const fraction = inputIndex - index;

        if (index + 1 < inputData.length) {
          output[i] = inputData[index] * (1 - fraction) + inputData[index + 1] * fraction;
        } else {
          output[i] = inputData[index];
        }
      }

      return output;
    }

    // Utility: Convert Float32 to PCM16
    function float32ToPcm16(float32Array) {
      const pcm16 = new Int16Array(float32Array.length);
      for (let i = 0; i < float32Array.length; i++) {
        const s = Math.max(-1, Math.min(1, float32Array[i]));
        pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
      }
      return pcm16;
    }

    // Utility: ArrayBuffer to Base64
    function arrayBufferToBase64(buffer) {
      const bytes = new Uint8Array(buffer);
      let binary = '';
      for (let i = 0; i < bytes.byteLength; i++) {
        binary += String.fromCharCode(bytes[i]);
      }
      return btoa(binary);
    }

    // End the interview
    function endInterview() {
      // Stop recording
      stopRecording();

      // Stop timer
      if (timerInterval) {
        clearInterval(timerInterval);
        timerInterval = null;
      }

      // Close WebSocket
      if (ws) {
        ws.close();
        ws = null;
      }

      // Stop media stream
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }

      // Close audio context
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }

      // Show complete screen
      document.getElementById('interview-screen').classList.add('hidden');
      document.getElementById('complete-screen').classList.remove('hidden');
      document.getElementById('final-transcript').textContent = transcript || '(No transcript recorded)';

      // Auto-download transcript
      if (transcript) {
        downloadTranscript();
      }
    }

    // Handle page unload
    window.addEventListener('beforeunload', () => {
      if (ws) ws.close();
      if (mediaStream) mediaStream.getTracks().forEach(track => track.stop());
      if (audioContext) audioContext.close();
    });
  </script>
</body>
</html>

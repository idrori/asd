<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Research Interview - ICIScopilot</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%);
      min-height: 100vh;
      color: #e2e8f0;
    }

    .container {
      max-width: 800px;
      margin: 0 auto;
      padding: 2rem;
    }

    .header {
      text-align: center;
      margin-bottom: 2rem;
    }

    .header h1 {
      font-size: 2rem;
      color: #10b981;
      margin-bottom: 0.5rem;
    }

    .header p {
      color: #94a3b8;
      font-size: 0.9rem;
    }

    .card {
      background: #1e293b;
      border-radius: 1rem;
      padding: 2rem;
      margin-bottom: 1.5rem;
      border: 1px solid #334155;
    }

    .form-group {
      margin-bottom: 1.5rem;
    }

    .form-group label {
      display: block;
      margin-bottom: 0.5rem;
      font-weight: 500;
      color: #cbd5e1;
    }

    .form-group input,
    .form-group textarea {
      width: 100%;
      padding: 0.75rem 1rem;
      border-radius: 0.5rem;
      border: 1px solid #475569;
      background: #0f172a;
      color: #e2e8f0;
      font-size: 1rem;
    }

    .form-group input:focus,
    .form-group textarea:focus {
      outline: none;
      border-color: #10b981;
    }

    .form-group textarea {
      min-height: 100px;
      resize: vertical;
    }

    .btn {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      gap: 0.5rem;
      padding: 0.75rem 1.5rem;
      border-radius: 0.5rem;
      font-size: 1rem;
      font-weight: 500;
      cursor: pointer;
      border: none;
      transition: all 0.2s;
    }

    .btn-primary {
      background: #10b981;
      color: white;
    }

    .btn-primary:hover {
      background: #059669;
    }

    .btn-primary:disabled {
      background: #475569;
      cursor: not-allowed;
    }

    .btn-danger {
      background: #ef4444;
      color: white;
    }

    .btn-danger:hover {
      background: #dc2626;
    }

    .btn-secondary {
      background: #475569;
      color: white;
    }

    .btn-secondary:hover {
      background: #64748b;
    }

    .interview-status {
      text-align: center;
      padding: 2rem;
    }

    .status-indicator {
      width: 120px;
      height: 120px;
      border-radius: 50%;
      margin: 0 auto 1.5rem;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 3rem;
    }

    .status-indicator.listening {
      background: rgba(239, 68, 68, 0.2);
      border: 3px solid #ef4444;
      animation: pulse 1.5s infinite;
    }

    .status-indicator.speaking {
      background: rgba(16, 185, 129, 0.2);
      border: 3px solid #10b981;
      animation: pulse 1s infinite;
    }

    .status-indicator.idle {
      background: rgba(148, 163, 184, 0.2);
      border: 3px solid #94a3b8;
    }

    .status-indicator.connecting {
      background: rgba(251, 191, 36, 0.2);
      border: 3px solid #fbbf24;
      animation: pulse 1s infinite;
    }

    @keyframes pulse {
      0%, 100% { transform: scale(1); opacity: 1; }
      50% { transform: scale(1.05); opacity: 0.8; }
    }

    .status-text {
      font-size: 1.25rem;
      margin-bottom: 0.5rem;
    }

    .status-subtext {
      color: #94a3b8;
      font-size: 0.9rem;
    }

    .transcript-box {
      background: #0f172a;
      border-radius: 0.5rem;
      padding: 1rem;
      max-height: 300px;
      overflow-y: auto;
      font-family: monospace;
      font-size: 0.85rem;
      line-height: 1.6;
      white-space: pre-wrap;
      border: 1px solid #334155;
    }

    .transcript-box .ai {
      color: #10b981;
    }

    .transcript-box .user {
      color: #60a5fa;
    }

    .controls {
      display: flex;
      gap: 1rem;
      justify-content: center;
      margin-top: 1.5rem;
    }

    .timer {
      font-size: 2rem;
      font-weight: bold;
      color: #10b981;
      text-align: center;
      margin-bottom: 1rem;
    }

    .hidden {
      display: none !important;
    }

    .error-box {
      background: rgba(239, 68, 68, 0.1);
      border: 1px solid #ef4444;
      border-radius: 0.5rem;
      padding: 1rem;
      color: #fca5a5;
      margin-bottom: 1rem;
    }

    .info-box {
      background: rgba(59, 130, 246, 0.1);
      border: 1px solid #3b82f6;
      border-radius: 0.5rem;
      padding: 1rem;
      color: #93c5fd;
      margin-bottom: 1rem;
      font-size: 0.9rem;
    }

    .complete-box {
      text-align: center;
      padding: 2rem;
    }

    .complete-box h2 {
      color: #10b981;
      margin-bottom: 1rem;
    }

    .powered-by {
      text-align: center;
      margin-top: 1rem;
      font-size: 0.8rem;
      color: #64748b;
    }
  </style>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>ICIScopilot Research Interview</h1>
      <p>Voice-based interview for automated science discovery</p>
    </div>

    <!-- Welcome Screen -->
    <div id="welcome-screen" class="card">
      <h2 style="margin-bottom: 1rem;">Welcome to Your Research Interview</h2>
      <p style="color: #94a3b8; margin-bottom: 1.5rem;">
        This is a structured 9-minute voice interview to understand your research needs.
        Please provide your details below, then click "Start Interview" to begin the voice conversation.
      </p>

      <div class="info-box">
        <strong>How it works:</strong> After you start, an AI interviewer will ask you questions about your research.
        Speak naturally - the AI uses voice activity detection to know when you're done speaking.
        At the end, a transcript will be automatically downloaded.
      </div>

      <div class="form-group">
        <label for="email">Email Address *</label>
        <input type="email" id="email" placeholder="your.email@university.edu" required>
      </div>

      <div class="form-group">
        <label for="references">Key References (1-3 papers that inspire your research)</label>
        <textarea id="references" placeholder="e.g., Smith et al. 2023 - 'AI in Healthcare'&#10;Johnson 2022 - 'Machine Learning Methods'"></textarea>
      </div>

      <div id="error-message" class="error-box hidden"></div>

      <button id="start-btn" class="btn btn-primary" onclick="startInterview()">
        Start Voice Interview
      </button>

      <div class="powered-by">Powered by OpenAI Realtime API (gpt-4o-realtime)</div>
    </div>

    <!-- Interview Screen -->
    <div id="interview-screen" class="card hidden">
      <div class="timer" id="timer">09:00</div>

      <div class="interview-status">
        <div class="status-indicator idle" id="status-indicator">
          <span id="status-icon">üéôÔ∏è</span>
        </div>
        <div class="status-text" id="status-text">Connecting...</div>
        <div class="status-subtext" id="status-subtext">Please wait while we set up the interview</div>
      </div>

      <h3 style="margin-bottom: 0.5rem;">Live Transcript</h3>
      <div class="transcript-box" id="transcript"></div>

      <div class="controls">
        <button class="btn btn-danger" onclick="endInterview()">
          End Interview
        </button>
      </div>
    </div>

    <!-- Complete Screen -->
    <div id="complete-screen" class="card hidden">
      <div class="complete-box">
        <h2>Interview Complete!</h2>
        <p style="color: #94a3b8; margin-bottom: 1.5rem;">
          Thank you for completing the research interview. Your transcript has been downloaded automatically.
        </p>

        <h3 style="margin-bottom: 0.5rem;">Final Transcript</h3>
        <div class="transcript-box" id="final-transcript" style="max-height: 400px;"></div>

        <div class="controls" style="margin-top: 1.5rem;">
          <button class="btn btn-secondary" onclick="downloadTranscript()">
            Download Transcript Again
          </button>
          <button class="btn btn-primary" onclick="window.close()">
            Close Window
          </button>
        </div>
      </div>
    </div>
  </div>

  <script>
    // Interview state
    let ws = null;
    let audioContext = null;
    let mediaStream = null;
    let scriptProcessor = null;
    let isRecording = false;
    let transcript = '';
    let userEmail = '';
    let userReferences = '';
    let interviewStartTime = null;
    let timerInterval = null;
    let currentAIText = '';
    let currentUserText = '';

    // Audio playback state
    let audioQueue = [];
    let isPlaying = false;
    let nextPlayTime = 0;

    // Get API base URL
    function getApiUrl() {
      if (window.location.hostname.includes('github.io')) {
        return 'https://icis-deploy-12-10-2025.vercel.app';
      }
      return window.location.origin;
    }

    // Show error message
    function showError(message) {
      const errorBox = document.getElementById('error-message');
      errorBox.textContent = message;
      errorBox.classList.remove('hidden');
    }

    // Hide error message
    function hideError() {
      document.getElementById('error-message').classList.add('hidden');
    }

    // Update status display
    function updateStatus(status, text, subtext) {
      const indicator = document.getElementById('status-indicator');
      const statusText = document.getElementById('status-text');
      const statusSubtext = document.getElementById('status-subtext');
      const icon = document.getElementById('status-icon');

      indicator.className = 'status-indicator ' + status;
      statusText.textContent = text;
      statusSubtext.textContent = subtext;

      if (status === 'listening') {
        icon.textContent = 'üéôÔ∏è';
      } else if (status === 'speaking') {
        icon.textContent = 'üîä';
      } else if (status === 'connecting') {
        icon.textContent = '‚è≥';
      } else {
        icon.textContent = 'üí¨';
      }
    }

    // Update timer display
    function updateTimer() {
      if (!interviewStartTime) return;

      const elapsed = Math.floor((Date.now() - interviewStartTime) / 1000);
      const remaining = Math.max(0, 9 * 60 - elapsed);
      const minutes = Math.floor(remaining / 60);
      const seconds = remaining % 60;

      document.getElementById('timer').textContent =
        `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;

      // Auto-end after 10 minutes
      if (elapsed >= 10 * 60) {
        endInterview();
      }
    }

    // Add to transcript
    function addToTranscript(speaker, text) {
      const transcriptBox = document.getElementById('transcript');
      const timestamp = new Date().toLocaleTimeString();

      if (speaker === 'AI') {
        transcript += `\n[${timestamp}] AI: ${text}`;
        transcriptBox.innerHTML += `<div class="ai">[${timestamp}] AI: ${text}</div>`;
      } else {
        transcript += `\n[${timestamp}] You: ${text}`;
        transcriptBox.innerHTML += `<div class="user">[${timestamp}] You: ${text}</div>`;
      }

      transcriptBox.scrollTop = transcriptBox.scrollHeight;
    }

    // Download transcript as text file
    function downloadTranscript() {
      const header = `Research Interview Transcript
Date: ${new Date().toLocaleString()}
Email: ${userEmail}
References: ${userReferences || 'None provided'}
${'='.repeat(50)}
`;

      const content = header + transcript;
      const blob = new Blob([content], { type: 'text/plain' });
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.href = url;
      a.download = `research_interview_${new Date().toISOString().split('T')[0]}.txt`;
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
      URL.revokeObjectURL(url);
    }

    // Convert Float32Array to Int16Array (PCM16)
    function floatTo16BitPCM(float32Array) {
      const int16Array = new Int16Array(float32Array.length);
      for (let i = 0; i < float32Array.length; i++) {
        const s = Math.max(-1, Math.min(1, float32Array[i]));
        int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
      }
      return int16Array;
    }

    // Convert Int16Array to base64
    function int16ToBase64(int16Array) {
      const bytes = new Uint8Array(int16Array.buffer);
      let binary = '';
      for (let i = 0; i < bytes.length; i++) {
        binary += String.fromCharCode(bytes[i]);
      }
      return btoa(binary);
    }

    // Resample audio from source rate to target rate
    function resampleAudio(audioData, sourceRate, targetRate) {
      if (sourceRate === targetRate) {
        return audioData;
      }
      const ratio = sourceRate / targetRate;
      const newLength = Math.round(audioData.length / ratio);
      const result = new Float32Array(newLength);
      for (let i = 0; i < newLength; i++) {
        const srcIndex = i * ratio;
        const srcIndexFloor = Math.floor(srcIndex);
        const srcIndexCeil = Math.min(srcIndexFloor + 1, audioData.length - 1);
        const t = srcIndex - srcIndexFloor;
        result[i] = audioData[srcIndexFloor] * (1 - t) + audioData[srcIndexCeil] * t;
      }
      return result;
    }

    // Play received audio using Web Audio API with proper scheduling
    async function playAudio(base64Data) {
      if (!audioContext) return;

      try {
        // Decode base64 to PCM16
        const binaryString = atob(base64Data);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }

        // Convert to Int16 then Float32
        const pcmData = new Int16Array(bytes.buffer);
        const floatData = new Float32Array(pcmData.length);
        for (let i = 0; i < pcmData.length; i++) {
          floatData[i] = pcmData[i] / 32768;
        }

        // Create audio buffer (OpenAI outputs 24kHz PCM16)
        const audioBuffer = audioContext.createBuffer(1, floatData.length, 24000);
        audioBuffer.getChannelData(0).set(floatData);

        // Schedule playback to avoid overlapping
        const currentTime = audioContext.currentTime;
        const startTime = Math.max(currentTime, nextPlayTime);

        const source = audioContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(audioContext.destination);

        // Update next play time
        nextPlayTime = startTime + audioBuffer.duration;

        source.start(startTime);

        // Update status while playing
        updateStatus('speaking', 'AI Speaking', 'Listening for your response...');

        source.onended = () => {
          // Check if all audio has played
          if (audioContext.currentTime >= nextPlayTime - 0.05) {
            updateStatus('listening', 'Your Turn', 'Speak when ready...');
          }
        };
      } catch (error) {
        console.error('Error playing audio:', error);
      }
    }

    // Start the interview
    async function startInterview() {
      hideError();

      userEmail = document.getElementById('email').value.trim();
      userReferences = document.getElementById('references').value.trim();

      if (!userEmail) {
        showError('Please enter your email address');
        return;
      }

      document.getElementById('start-btn').disabled = true;
      document.getElementById('start-btn').textContent = 'Connecting...';

      try {
        // Get ephemeral token from server
        const apiUrl = getApiUrl();
        console.log('[Interview] Fetching token from:', apiUrl);

        const tokenResponse = await fetch(`${apiUrl}/api/openai-realtime-token`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' }
        });

        if (!tokenResponse.ok) {
          const errorData = await tokenResponse.json();
          throw new Error(errorData.error || 'Failed to get session token');
        }

        const tokenData = await tokenResponse.json();
        console.log('[Interview] Got session token');

        if (!tokenData.success || !tokenData.client_secret) {
          throw new Error('Invalid token response');
        }

        // Initialize audio context for playback
        audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 24000 });
        nextPlayTime = audioContext.currentTime;

        // Request microphone access
        mediaStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            sampleRate: 24000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });

        // Connect to OpenAI Realtime API via WebSocket
        const model = 'gpt-4o-realtime-preview-2024-12-17';
        const wsUrl = `wss://api.openai.com/v1/realtime?model=${model}`;

        console.log('[Interview] Connecting to WebSocket...');

        ws = new WebSocket(wsUrl, [
          'realtime',
          `openai-insecure-api-key.${tokenData.client_secret.value}`
        ]);

        ws.onopen = () => {
          console.log('[Interview] WebSocket connected');

          // Switch to interview screen
          document.getElementById('welcome-screen').classList.add('hidden');
          document.getElementById('interview-screen').classList.remove('hidden');

          updateStatus('connecting', 'Connected', 'Starting interview...');

          // Start timer
          interviewStartTime = Date.now();
          timerInterval = setInterval(updateTimer, 1000);

          // Send session update with our configuration
          ws.send(JSON.stringify({
            type: 'session.update',
            session: {
              modalities: ['text', 'audio'],
              voice: 'alloy',
              input_audio_format: 'pcm16',
              output_audio_format: 'pcm16',
              input_audio_transcription: {
                model: 'whisper-1'
              },
              turn_detection: {
                type: 'server_vad',
                threshold: 0.5,
                prefix_padding_ms: 300,
                silence_duration_ms: 500,
                create_response: true
              }
            }
          }));

          // Start audio recording after a short delay
          setTimeout(() => {
            startAudioRecording();
            // Request initial greeting from AI
            ws.send(JSON.stringify({
              type: 'response.create',
              response: {
                modalities: ['text', 'audio'],
                instructions: 'Please greet the participant warmly and begin the research interview by introducing yourself and explaining the process.'
              }
            }));
          }, 500);
        };

        ws.onmessage = (event) => {
          try {
            const data = JSON.parse(event.data);
            handleServerEvent(data);
          } catch (error) {
            console.error('[Interview] Error parsing message:', error);
          }
        };

        ws.onerror = (error) => {
          console.error('[Interview] WebSocket error:', error);
          showError('Connection error. Please try again.');
          cleanup();
        };

        ws.onclose = (event) => {
          console.log('[Interview] WebSocket closed:', event.code, event.reason);
          // Only end interview if we were actually in an active interview
          // Code 1000 = normal close, 1006 = abnormal (connection lost)
          if (isRecording && interviewStartTime) {
            // If interview just started (< 5 seconds), show error instead of ending
            const elapsed = Date.now() - interviewStartTime;
            if (elapsed < 5000) {
              console.error('[Interview] Connection closed immediately after start');
              showError(`Connection closed: ${event.reason || 'Unknown reason'} (code: ${event.code})`);
              cleanup();
              document.getElementById('interview-screen').classList.add('hidden');
              document.getElementById('welcome-screen').classList.remove('hidden');
              document.getElementById('start-btn').disabled = false;
              document.getElementById('start-btn').textContent = 'Start Voice Interview';
            } else {
              endInterview();
            }
          }
        };

      } catch (error) {
        console.error('[Interview] Error:', error);
        showError(error.message || 'Failed to start interview');
        document.getElementById('start-btn').disabled = false;
        document.getElementById('start-btn').textContent = 'Start Voice Interview';
        cleanup();
      }
    }

    // Handle server events
    function handleServerEvent(event) {
      switch (event.type) {
        case 'session.created':
        case 'session.updated':
          console.log('[Interview] Session ready');
          updateStatus('listening', 'Ready', 'Waiting for AI to speak...');
          break;

        case 'response.audio.delta':
          // Received audio chunk from AI
          if (event.delta) {
            playAudio(event.delta);
          }
          break;

        case 'response.audio_transcript.delta':
          // AI speech transcription (streaming)
          if (event.delta) {
            currentAIText += event.delta;
          }
          break;

        case 'response.audio_transcript.done':
          // AI finished speaking - add to transcript
          if (currentAIText) {
            addToTranscript('AI', currentAIText);
            currentAIText = '';
          }
          break;

        case 'conversation.item.input_audio_transcription.completed':
          // User's speech was transcribed
          if (event.transcript) {
            addToTranscript('User', event.transcript);
          }
          break;

        case 'input_audio_buffer.speech_started':
          updateStatus('listening', 'Listening...', 'You are speaking');
          break;

        case 'input_audio_buffer.speech_stopped':
          updateStatus('idle', 'Processing...', 'AI is thinking');
          break;

        case 'response.created':
          console.log('[Interview] Response started');
          break;

        case 'response.done':
          console.log('[Interview] Response complete');
          // Flush any remaining AI text
          if (currentAIText) {
            addToTranscript('AI', currentAIText);
            currentAIText = '';
          }
          updateStatus('listening', 'Your Turn', 'Speak when ready...');
          break;

        case 'error':
          console.error('[Interview] Server error:', event.error);
          const errorMsg = event.error?.message || event.error?.code || JSON.stringify(event.error);
          showError(`Error: ${errorMsg}`);
          break;

        default:
          // Log other events for debugging
          if (event.type && !event.type.includes('delta')) {
            console.log('[Interview] Event:', event.type);
          }
      }
    }

    // Start recording audio from microphone
    function startAudioRecording() {
      if (!mediaStream || !ws) return;

      isRecording = true;

      // Create audio context for recording at mic sample rate
      const recordingContext = new (window.AudioContext || window.webkitAudioContext)();
      const source = recordingContext.createMediaStreamSource(mediaStream);

      // Use ScriptProcessor for audio capture (deprecated but widely supported)
      // Buffer size of 4096 at 24kHz = ~170ms chunks
      const bufferSize = 4096;
      scriptProcessor = recordingContext.createScriptProcessor(bufferSize, 1, 1);

      scriptProcessor.onaudioprocess = (e) => {
        if (!isRecording || !ws || ws.readyState !== WebSocket.OPEN) return;

        const inputData = e.inputBuffer.getChannelData(0);

        // Resample to 24kHz if needed
        const resampledData = resampleAudio(inputData, recordingContext.sampleRate, 24000);

        // Convert to PCM16 and base64
        const pcm16 = floatTo16BitPCM(resampledData);
        const base64Audio = int16ToBase64(pcm16);

        // Send audio to OpenAI
        ws.send(JSON.stringify({
          type: 'input_audio_buffer.append',
          audio: base64Audio
        }));
      };

      source.connect(scriptProcessor);
      scriptProcessor.connect(recordingContext.destination);

      console.log('[Interview] Audio recording started');
      updateStatus('listening', 'Ready', 'Listening for AI greeting...');
    }

    // End the interview
    function endInterview() {
      console.log('[Interview] Ending interview');
      isRecording = false;

      // Send any remaining audio and close
      if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({
          type: 'input_audio_buffer.commit'
        }));
        ws.close();
      }

      cleanup();

      // Download transcript
      downloadTranscript();

      // Show completion screen
      document.getElementById('interview-screen').classList.add('hidden');
      document.getElementById('complete-screen').classList.remove('hidden');
      document.getElementById('final-transcript').innerHTML = document.getElementById('transcript').innerHTML;
    }

    // Cleanup resources
    function cleanup() {
      if (timerInterval) {
        clearInterval(timerInterval);
        timerInterval = null;
      }

      if (scriptProcessor) {
        scriptProcessor.disconnect();
        scriptProcessor = null;
      }

      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }

      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }

      if (ws) {
        ws.close();
        ws = null;
      }
    }

    // Cleanup on page unload
    window.addEventListener('beforeunload', cleanup);
  </script>
</body>
</html>
